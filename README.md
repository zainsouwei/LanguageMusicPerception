# LanguageMusicPerception

This repository contains code used in the data analysis for a pilot study investigating the relationship between language and music perception. Full details of the pilot study can be found in the PilotStudy.pdf file.

Abstract
The theory that language and music perception have the ability to influence each other is being continuously supported by studies across many disciplines. Studies have shown that there are shared acoustic features that encode emotions in speech and in music. This pilot study builds on these ideas by investigating whether features in a person's speech are correlated with the acoustic features in their preferred music by representing each in the same feature space. To do this, speech recordings from seven participants reading a provided elicitation passage were collected. The participants were then asked to like 10-25 second samples from 30 songs. Correlations between acoustic features in the participantsâ€™ speech recordings and the songs they liked were then calculated. No significant correlations were found between features in speech and preferred music. Due to the small sample size, no substantial conclusions can be drawn from this pilot study.
